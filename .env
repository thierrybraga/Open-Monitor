FLASK_ENV=development
FLASK_APP=app.app:create_app
FLASK_RUN_PORT=4443
PUBLIC_MODE=true
LOGIN_ENABLED_IN_PUBLIC_MODE=true
SECRET_KEY=2f1b4d2f6e9a4b3d8c7e2a1f9b0c4a7d6e5f3a2b1c4d7e8f9a0b1c2d3e4f5a6
LOG_LEVEL=DEBUG
LOG_FILE=logs/openmonitor.log
SESSION_COOKIE_SAMESITE=Lax
SESSION_COOKIE_SECURE=false
SESSION_LIFETIME_MINUTES=30
ANALYTICS_CACHE_TTL=900
ANALYTICS_CACHE_REFRESH_INTERVAL_MINUTES=15
MULTILINGUAL=true
BABEL_DEFAULT_TIMEZONE=UTC
PDF_ENGINE=reportlab
BASE_URL=http://localhost:4443

# NVD API (BaseConfig)
NVD_API_BASE=https://services.nvd.nist.gov/rest/json/cves/2.0
NVD_REQUEST_TIMEOUT=30
NVD_USER_AGENT=Open-Monitor NVD Fetcher
NVD_API_KEY=05e0fc86-d393-4ca5-9e64-858673553c05

# Parallel NVD Config (from_env)
NVD_USE_ENHANCED=true
NVD_MAX_WORKERS=5
NVD_BATCH_SIZE=500
MAX_CONCURRENT_REQUESTS=10
NVD_ENABLE_CACHE=false
REDIS_CACHE_TTL=3600
REDIS_CACHE_PREFIX=nvd_cache:
NVD_ENABLE_MONITORING=true
PERFORMANCE_MONITORING_INTERVAL=60
PERFORMANCE_LOG_LEVEL=INFO
NVD_FALLBACK_ON_ERROR=true

# Redis (optional)
REDIS_CACHE_ENABLED=false
REDIS_URL=redis://localhost:6379/0
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# OpenAI (optional)
OPENAI_API_KEY=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
OPENAI_MODEL=gpt-5.1
OPENAI_MAX_TOKENS=1000
OPENAI_TEMPERATURE=0.7
OPENAI_TIMEOUT=30
OPENAI_MAX_RETRIES=2
OPENAI_RETRY_BACKOFF=1.5
OPENAI_STREAMING=0

LLM_COMPLETION_TOKENS_PARAM=max_tokens

# LLM/OpenAI advanced parameters
LLM_PROVIDER=openai
OPENAI_TOP_P=0.9
OPENAI_PRESENCE_PENALTY=0.0
OPENAI_FREQUENCY_PENALTY=0.0
OPENAI_STOP=
LLM_SYSTEM_PROMPT=Você é um analista de risco especializado em vulnerabilidades. Produza um relatório técnico claro, objetivo, em Markdown, com recomendações acionáveis.

# Modelos compatíveis com 'max_tokens' (ex.: gpt-4o-mini)
# OPENAI_MODEL=gpt-4o-mini

# Modelos >5 (ex.: gpt-5.1) utilizam 'max_completion_tokens' internamente
# OPENAI_MODEL=gpt-5.1
# Observação: OPENAI_MAX_TOKENS será usado como 'max_completion_tokens' em modelos >5

# LLM genérica
# LLM_PROVIDER=openai
# LLM_BASE_URL=
# LLM_API_KEY=
# LLM_MODEL=
# LLM_COMPLETION_TOKENS_PARAM=max_tokens

# DeepSeek (OpenAI-compatible)
# LLM_PROVIDER=deepseek
# LLM_BASE_URL=https://api.deepseek.com/v1
# LLM_API_KEY=your_deepseek_api_key
# LLM_MODEL=deepseek-chat
# LLM_COMPLETION_TOKENS_PARAM=max_tokens

# Google Gemini (REST)
# LLM_PROVIDER=gemini
# LLM_API_KEY=your_gemini_api_key
# LLM_MODEL=gemini-1.5-flash

# LM Studio (local, OpenAI-compatible)
# LLM_PROVIDER=lmstudio
# LLM_BASE_URL=http://localhost:1234/v1
# LLM_API_KEY=lm-studio
# LLM_MODEL=local-model
# LLM_COMPLETION_TOKENS_PARAM=max_tokens

# Server
PORT=4443
