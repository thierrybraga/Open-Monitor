#!/usr/bin/env python3
"""
Script de sincroniza√ß√£o NVD com processamento paralelo.
Integra o ParallelNVDService com o sistema existente.
"""

import sys
import os
import asyncio
import logging
import argparse
from datetime import datetime
from pathlib import Path

# Adicionar o diret√≥rio pai ao path para importa√ß√µes
sys.path.insert(0, str(Path(__file__).parent.parent))

from flask import Flask
from app.extensions import db
from app.models.sync_metadata import SyncMetadata
from services.parallel_nvd_service import ParallelNVDService
from services.redis_cache_service import RedisCacheService
from services.vulnerability_service import VulnerabilityService
from app.config.scheduler_config import SchedulerConfig

logger = logging.getLogger(__name__)

def setup_logging(debug: bool = False):
    """Configura logging para o script."""
    level = logging.DEBUG if debug else logging.INFO
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('logs/parallel_nvd_sync.log', mode='a')
        ]
    )
    
    # Reduzir verbosidade de bibliotecas externas
    logging.getLogger('aiohttp').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)

def create_app():
    """Cria inst√¢ncia da aplica√ß√£o Flask para contexto de banco."""
    app = Flask(__name__)
    
    # Carregar configura√ß√µes
    from settings.development import DevelopmentConfig
    app.config.from_object(DevelopmentConfig)
    
    # Inicializar extens√µes
    db.init_app(app)
    
    return app

def get_nvd_config(app):
    """Extrai configura√ß√µes NVD da aplica√ß√£o."""
    return {
        'NVD_API_BASE': getattr(app.config, 'NVD_API_BASE', 'https://services.nvd.nist.gov/rest/json/cves/2.0'),
        'NVD_API_KEY': getattr(app.config, 'NVD_API_KEY', None),
        'NVD_PAGE_SIZE': getattr(app.config, 'NVD_PAGE_SIZE', 2000),
        'NVD_REQUEST_TIMEOUT': getattr(app.config, 'NVD_REQUEST_TIMEOUT', 30),
        'NVD_USER_AGENT': getattr(app.config, 'NVD_USER_AGENT', 'Sec4all.co Parallel NVD Fetcher'),
        'NVD_MAX_RETRIES': getattr(app.config, 'NVD_MAX_RETRIES', 3),
        'BATCH_SIZE': getattr(app.config, 'NVD_BATCH_SIZE', 100),
        'DB_BATCH_SIZE': getattr(app.config, 'NVD_DB_BATCH_SIZE', 500),
        'MAX_CONCURRENT_REQUESTS': getattr(app.config, 'NVD_MAX_CONCURRENT_REQUESTS', 5)
    }

def get_redis_config(app):
    """Extrai configura√ß√µes Redis da aplica√ß√£o."""
    return {
        'REDIS_CACHE_ENABLED': app.config.get('REDIS_CACHE_ENABLED', True),
        'REDIS_URL': app.config.get('REDIS_URL', 'redis://localhost:6379/0'),
        'REDIS_HOST': app.config.get('REDIS_HOST', 'localhost'),
        'REDIS_PORT': app.config.get('REDIS_PORT', 6379),
        'REDIS_DB': app.config.get('REDIS_DB', 0),
        'REDIS_PASSWORD': app.config.get('REDIS_PASSWORD'),
        'CACHE_DEFAULT_TTL': app.config.get('CACHE_DEFAULT_TTL', 3600),
        'CACHE_MAX_TTL': app.config.get('CACHE_MAX_TTL', 86400),
        'CACHE_KEY_PREFIX': app.config.get('CACHE_KEY_PREFIX', 'nvd_cache:'),
        'CACHE_USE_COMPRESSION': app.config.get('CACHE_USE_COMPRESSION', True),
        'CACHE_COMPRESSION_THRESHOLD': app.config.get('CACHE_COMPRESSION_THRESHOLD', 1024)
    }

async def run_parallel_sync(full_sync: bool = False, max_concurrent: int = 5, 
                          use_cache: bool = True, debug: bool = False):
    """
    Executa sincroniza√ß√£o paralela da API NVD.
    
    Args:
        full_sync: Se True, faz sincroniza√ß√£o completa
        max_concurrent: N√∫mero m√°ximo de requisi√ß√µes concorrentes
        use_cache: Se True, usa cache Redis
        debug: Se True, ativa logs de debug
    """
    setup_logging(debug)
    
    logger.info("=== Iniciando Sincroniza√ß√£o Paralela NVD ===")
    logger.info(f"Modo: {'Completo' if full_sync else 'Incremental'}")
    logger.info(f"Requisi√ß√µes concorrentes: {max_concurrent}")
    logger.info(f"Cache Redis: {'Ativado' if use_cache else 'Desativado'}")
    
    # Criar aplica√ß√£o Flask
    app = create_app()
    
    with app.app_context():
        try:
            # Obter configura√ß√µes
            nvd_config = get_nvd_config(app)
            nvd_config['MAX_CONCURRENT_REQUESTS'] = max_concurrent
            
            redis_config = get_redis_config(app)
            redis_config['REDIS_CACHE_ENABLED'] = use_cache
            
            # Inicializar servi√ßos
            logger.info("Inicializando servi√ßos...")
            
            # Cache Redis
            cache_service = RedisCacheService(redis_config)
            cache_info = cache_service.get_cache_info()
            logger.info(f"Cache Redis: {cache_info.get('enabled', False)}")
            
            # Servi√ßo de vulnerabilidades
            from sqlalchemy.orm import sessionmaker
            Session = sessionmaker(bind=db.engine)
            session = Session()
            vulnerability_service = VulnerabilityService(session)
            
            # Servi√ßo de processamento paralelo
            parallel_service = ParallelNVDService(nvd_config, max_concurrent)
            
            # Verificar √∫ltima sincroniza√ß√£o
            if not full_sync:
                last_sync = vulnerability_service.get_last_sync_time()
                if last_sync:
                    logger.info(f"√öltima sincroniza√ß√£o: {last_sync}")
                else:
                    logger.info("Nenhuma sincroniza√ß√£o anterior encontrada, executando sync completo")
                    full_sync = True
            
            # Executar sincroniza√ß√£o paralela
            logger.info("Iniciando processamento paralelo...")
            start_time = datetime.now()
            
            metrics = await parallel_service.parallel_sync(
                full_sync=full_sync,
                vulnerability_service=vulnerability_service
            )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # Relat√≥rio de performance
            logger.info("=== Relat√≥rio de Performance ===")
            performance_report = parallel_service.get_performance_report()
            
            for key, value in performance_report.items():
                logger.info(f"{key}: {value}")
            
            # Estat√≠sticas do cache
            if use_cache:
                cache_stats = cache_service.get_cache_info()
                logger.info("=== Estat√≠sticas do Cache ===")
                if 'stats' in cache_stats:
                    for key, value in cache_stats['stats'].items():
                        logger.info(f"Cache {key}: {value}")
            
            # Resumo final
            logger.info("=== Resumo Final ===")
            logger.info(f"Dura√ß√£o total: {duration:.2f} segundos")
            logger.info(f"CVEs processados: {metrics.total_cves_processed}")
            logger.info(f"CVEs salvos: {metrics.total_cves_saved}")
            logger.info(f"Taxa de sucesso: {metrics.success_rate:.1f}%")
            logger.info(f"Performance: {metrics.cves_per_second:.2f} CVEs/segundo")
            
            if metrics.total_cves_saved > 0:
                logger.info("‚úÖ Sincroniza√ß√£o conclu√≠da com sucesso!")
                return True
            else:
                logger.warning("‚ö†Ô∏è Nenhum CVE foi salvo")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro durante sincroniza√ß√£o: {e}", exc_info=True)
            return False

def benchmark_performance(iterations: int = 3):
    """
    Executa benchmark de performance comparando diferentes configura√ß√µes.
    
    Args:
        iterations: N√∫mero de itera√ß√µes para cada teste
    """
    logger.info("=== Iniciando Benchmark de Performance ===")
    
    # Configura√ß√µes para testar
    test_configs = [
        {'concurrent': 1, 'cache': False, 'name': 'Sequencial sem cache'},
        {'concurrent': 1, 'cache': True, 'name': 'Sequencial com cache'},
        {'concurrent': 3, 'cache': False, 'name': 'Paralelo (3) sem cache'},
        {'concurrent': 3, 'cache': True, 'name': 'Paralelo (3) com cache'},
        {'concurrent': 5, 'cache': True, 'name': 'Paralelo (5) com cache'},
        {'concurrent': 10, 'cache': True, 'name': 'Paralelo (10) com cache'}
    ]
    
    results = []
    
    for config in test_configs:
        logger.info(f"\n--- Testando: {config['name']} ---")
        
        config_results = []
        
        for i in range(iterations):
            logger.info(f"Itera√ß√£o {i+1}/{iterations}")
            
            start_time = datetime.now()
            
            # Executar teste (apenas incremental para benchmark)
            success = asyncio.run(run_parallel_sync(
                full_sync=False,
                max_concurrent=config['concurrent'],
                use_cache=config['cache'],
                debug=False
            ))
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            config_results.append({
                'duration': duration,
                'success': success
            })
            
            logger.info(f"Dura√ß√£o: {duration:.2f}s, Sucesso: {success}")
        
        # Calcular estat√≠sticas
        successful_runs = [r for r in config_results if r['success']]
        if successful_runs:
            avg_duration = sum(r['duration'] for r in successful_runs) / len(successful_runs)
            min_duration = min(r['duration'] for r in successful_runs)
            max_duration = max(r['duration'] for r in successful_runs)
            
            results.append({
                'config': config['name'],
                'avg_duration': avg_duration,
                'min_duration': min_duration,
                'max_duration': max_duration,
                'success_rate': len(successful_runs) / len(config_results) * 100
            })
        
    # Relat√≥rio final
    logger.info("\n=== Relat√≥rio de Benchmark ===")
    logger.info(f"{'Configura√ß√£o':<25} {'M√©dia (s)':<12} {'M√≠n (s)':<10} {'M√°x (s)':<10} {'Taxa Sucesso':<12}")
    logger.info("-" * 80)
    
    for result in results:
        logger.info(
            f"{result['config']:<25} "
            f"{result['avg_duration']:<12.2f} "
            f"{result['min_duration']:<10.2f} "
            f"{result['max_duration']:<10.2f} "
            f"{result['success_rate']:<12.1f}%"
        )
    
    # Encontrar melhor configura√ß√£o
    if results:
        best_config = min(results, key=lambda x: x['avg_duration'])
        logger.info(f"\nüèÜ Melhor configura√ß√£o: {best_config['config']}")
        logger.info(f"   Tempo m√©dio: {best_config['avg_duration']:.2f}s")

def main():
    """Fun√ß√£o principal do script."""
    parser = argparse.ArgumentParser(
        description='Sincroniza√ß√£o paralela da API NVD',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python parallel_nvd_sync.py --full                    # Sincroniza√ß√£o completa
  python parallel_nvd_sync.py --incremental             # Sincroniza√ß√£o incremental
  python parallel_nvd_sync.py --concurrent 10 --cache   # 10 requisi√ß√µes paralelas com cache
  python parallel_nvd_sync.py --benchmark               # Executar benchmark
  python parallel_nvd_sync.py --debug                   # Modo debug
        """
    )
    
    # Argumentos principais
    sync_group = parser.add_mutually_exclusive_group(required=True)
    sync_group.add_argument(
        '--full', 
        action='store_true',
        help='Executa sincroniza√ß√£o completa (todos os CVEs)'
    )
    sync_group.add_argument(
        '--incremental', 
        action='store_true',
        help='Executa sincroniza√ß√£o incremental (apenas CVEs modificados)'
    )
    sync_group.add_argument(
        '--benchmark', 
        action='store_true',
        help='Executa benchmark de performance'
    )
    
    # Configura√ß√µes de performance
    parser.add_argument(
        '--concurrent', 
        type=int, 
        default=5,
        help='N√∫mero m√°ximo de requisi√ß√µes concorrentes (padr√£o: 5)'
    )
    parser.add_argument(
        '--cache', 
        action='store_true',
        help='Ativa cache Redis (padr√£o: desativado)'
    )
    parser.add_argument(
        '--no-cache', 
        action='store_true',
        help='Desativa cache Redis explicitamente'
    )
    
    # Configura√ß√µes de debug
    parser.add_argument(
        '--debug', 
        action='store_true',
        help='Ativa logs de debug'
    )
    parser.add_argument(
        '--benchmark-iterations', 
        type=int, 
        default=3,
        help='N√∫mero de itera√ß√µes para benchmark (padr√£o: 3)'
    )
    
    args = parser.parse_args()
    
    # Validar argumentos
    if args.concurrent < 1 or args.concurrent > 20:
        parser.error("--concurrent deve estar entre 1 e 20")
    
    if args.benchmark_iterations < 1 or args.benchmark_iterations > 10:
        parser.error("--benchmark-iterations deve estar entre 1 e 10")
    
    # Determinar configura√ß√£o de cache
    use_cache = args.cache and not args.no_cache
    
    try:
        if args.benchmark:
            # Executar benchmark
            benchmark_performance(args.benchmark_iterations)
        else:
            # Executar sincroniza√ß√£o
            success = asyncio.run(run_parallel_sync(
                full_sync=args.full,
                max_concurrent=args.concurrent,
                use_cache=use_cache,
                debug=args.debug
            ))
            
            sys.exit(0 if success else 1)
            
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Sincroniza√ß√£o interrompida pelo usu√°rio")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Erro fatal: {e}", exc_info=True)
        sys.exit(1)

if __name__ == '__main__':
    main()