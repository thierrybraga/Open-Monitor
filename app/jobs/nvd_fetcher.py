
import sys
import os
import time
import pickle
import asyncio
import logging
import argparse
import re
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional # Importar Optional explicitamente
from pathlib import Path # Importar Path explicitamente

import aiohttp
import tqdm
from flask import Flask # current_app n√£o √© necess√°rio aqui, pois estamos em um contexto de job standalone
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import Session

# Sistema de feedback aprimorado
from app.utils.terminal_feedback import terminal_feedback, timed_operation
from app.utils.visual_indicators import status_indicator
from app.utils.enhanced_logging import get_app_logger
from app.utils.memory_monitor import memory_monitor
from app.utils.nvd_statistics import nvd_stats

# Use importa√ß√µes relativas CORRETAS para m√≥dulos DENTRO do pacote project
# Assumindo que o pacote 'project' √© o diret√≥rio pai de 'jobs'
from app.extensions import db # Importa db do pacote extensions (__init__.py)
# Importa√ß√µes CORRETAS dos modelos (Assumindo que est√£o em project/models)
from app.models.sync_metadata import SyncMetadata
from app.models.vulnerability import Vulnerability
from app.models.cvss_metric import CVSSMetric
# from models.api_call_log import ApiCallLog # Importar se o modelo for usado aqui
# from services.vulnerability_service import VulnerabilityService # Importar ap√≥s criar o servi√ßo
from app.jobs.nvd_enhancements import CWEAutoMapper, EnhancedReferenceProcessor
from app.utils.rate_limiter import NVDRateLimiter
from app.utils.severity_mapper import map_cvss_score_to_severity, get_primary_severity_from_metrics

logger = logging.getLogger(__name__)

# TODO: Garantir que estas configura√ß√µes sejam carregadas da configura√ß√£o da aplica√ß√£o Flask (app.config)
# A classe NVDFetcher deve receb√™-las via __init__.
# CACHE_DIR = Path('cache') # O caminho do cache deve ser configur√°vel (obtido da config)
# API_BASE    = os.getenv("NVD_API_BASE", "https://services.nvd.nist.gov/rest/json/cves/2.0") # Obtido da config
# API_KEY     = os.getenv("NVD_API_KEY", None) # Obtido da config
# PAGE_SIZE   = 2000 # Obtido da config
# MAX_RETRIES = 5 # Obtido da config
# RATE_LIMIT  = (2, 1)  # 2 requests per 1 sec (para a API Key Gratuita) # Obtido da config


class EnhancedCPEParser:
    """Parser aprimorado para extrair vendor/product de m√∫ltiplas fontes"""
    
    def __init__(self):
        # Padr√µes conhecidos de vendors
        self.vendor_patterns = {
            r'\bmicrosoft\b': 'microsoft',
            r'\bapple\b': 'apple',
            r'\boracle\b': 'oracle',
            r'\bgoogle\b': 'google',
            r'\bdebian\b': 'debian',
            r'\bubuntu\b': 'canonical',
            r'\bredhat\b': 'redhat',
            r'\brhel\b': 'redhat',
            r'\bcentos\b': 'centos',
            r'\bfedora\b': 'fedoraproject',
            r'\bsuse\b': 'suse',
            r'\bopensuse\b': 'opensuse',
            r'\bcisco\b': 'cisco',
            r'\bibm\b': 'ibm',
            r'\bintel\b': 'intel',
            r'\bamd\b': 'amd',
            r'\bnvidia\b': 'nvidia',
            r'\badobe\b': 'adobe',
            r'\bmozilla\b': 'mozilla',
            r'\bfirefox\b': 'mozilla',
            r'\bchrome\b': 'google',
            r'\bchromium\b': 'google',
            r'\blinux\b': 'linux',
            r'\bwindows\b': 'microsoft',
            r'\bmacos\b': 'apple',
            r'\bios\b': 'apple',
            r'\bandroid\b': 'google',
            r'\bjava\b': 'oracle',
            r'\bopenjdk\b': 'openjdk',
            r'\bphp\b': 'php',
            r'\bpython\b': 'python',
            r'\bnodejs\b': 'nodejs',
            r'\bnode\.js\b': 'nodejs',
            r'\bnpm\b': 'npmjs',
            r'\byarn\b': 'yarnpkg',
            r'\bdocker\b': 'docker',
            r'\bkubernetes\b': 'kubernetes',
            r'\bjenkins\b': 'jenkins',
            r'\bgit\b': 'git-scm',
            r'\bapache\b': 'apache',
            r'\bnginx\b': 'nginx',
            r'\bmysql\b': 'mysql',
            r'\bpostgresql\b': 'postgresql',
            r'\bmongodb\b': 'mongodb',
            r'\bredis\b': 'redis',
            r'\belasticsearch\b': 'elastic',
            r'\bkibana\b': 'elastic',
            r'\blogstash\b': 'elastic',
            r'\bsplunk\b': 'splunk',
            r'\bvmware\b': 'vmware',
            r'\bcitrix\b': 'citrix',
            r'\baws\b': 'amazon',
            r'\bamazon\b': 'amazon',
            r'\bazure\b': 'microsoft',
            r'\bgcp\b': 'google',
            r'\bwordpress\b': 'wordpress',
            r'\bdrupal\b': 'drupal',
            r'\bjoomla\b': 'joomla',
            r'\bmagento\b': 'magento',
            r'\bshopify\b': 'shopify',
            r'\bsalesforce\b': 'salesforce',
            r'\bslack\b': 'slack',
            r'\bzoom\b': 'zoom',
            r'\bteams\b': 'microsoft',
            r'\bskype\b': 'microsoft',
            r'\bwhatsapp\b': 'whatsapp',
            r'\btelegram\b': 'telegram',
            r'\bdiscord\b': 'discord'
        }
        
        # Padr√µes conhecidos de products
        self.product_patterns = {
            r'\bwindows\s+\d+': 'windows',
            r'\bwindows\s+server': 'windows_server',
            r'\boffice\s+\d+': 'office',
            r'\bexchange\s+server': 'exchange_server',
            r'\bsql\s+server': 'sql_server',
            r'\bvisual\s+studio': 'visual_studio',
            r'\b\.net\s+framework': 'dotnet_framework',
            r'\biis': 'iis',
            r'\bmacos': 'macos',
            r'\bios': 'ios',
            r'\bsafari': 'safari',
            r'\bitunes': 'itunes',
            r'\bxcode': 'xcode',
            r'\bfirefox': 'firefox',
            r'\bthunderbird': 'thunderbird',
            r'\bchrome': 'chrome',
            r'\bchromium': 'chromium',
            r'\bandroid': 'android',
            r'\bgmail': 'gmail',
            r'\byoutube': 'youtube',
            r'\bmaps': 'maps',
            r'\bdrive': 'drive',
            r'\blinux\s+kernel': 'linux_kernel',
            r'\bdebian': 'debian',
            r'\bubuntu': 'ubuntu',
            r'\bcentos': 'centos',
            r'\bfedora': 'fedora',
            r'\brhel': 'rhel',
            r'\bsuse': 'suse',
            r'\bopensuse': 'opensuse',
            r'\bapache\s+http\s+server': 'apache_httpd',
            r'\bapache\s+tomcat': 'tomcat',
            r'\bnginx': 'nginx',
            r'\bmysql': 'mysql',
            r'\bpostgresql': 'postgresql',
            r'\bmongodb': 'mongodb',
            r'\bredis': 'redis',
            r'\belasticsearch': 'elasticsearch',
            r'\bkibana': 'kibana',
            r'\blogstash': 'logstash',
            r'\bjenkins': 'jenkins',
            r'\bdocker': 'docker',
            r'\bkubernetes': 'kubernetes',
            r'\bwordpress': 'wordpress',
            r'\bdrupal': 'drupal',
            r'\bjoomla': 'joomla',
            r'\bmagento': 'magento',
            r'\bphp': 'php',
            r'\bpython': 'python',
            r'\bnodejs': 'nodejs',
            r'\bnode\.js': 'nodejs',
            r'\bnpm': 'npm',
            r'\byarn': 'yarn',
            r'\bjava': 'java',
            r'\bopenjdk': 'openjdk'
        }
    
    def extract_from_cpe(self, configurations):
        """Extrai vendors, products e informa√ß√µes de vers√£o de configura√ß√µes CPE"""
        vendors = set()
        products = set()
        version_ranges = []
        
        for config in configurations:
            for node in config.get('nodes', []):
                for cpe_match in node.get('cpeMatch', []):
                    cpe_uri = cpe_match.get('criteria', '')
                    vulnerable = cpe_match.get('vulnerable', False)
                    
                    if cpe_uri.startswith('cpe:2.3:'):
                        parts = cpe_uri.split(':')
                        if len(parts) >= 5:
                            vendor = parts[3]
                            product = parts[4]
                            version = parts[5] if len(parts) > 5 else None
                            
                            if vendor and vendor != '*' and vendor != '-':
                                vendor = re.sub(r'[^a-zA-Z0-9_-]', '', vendor)
                                if vendor:
                                    vendors.add(vendor.lower())
                            
                            if product and product != '*' and product != '-':
                                product = re.sub(r'[^a-zA-Z0-9_-]', '', product)
                                if product:
                                    products.add(product.lower())
                            
                            # Extrair informa√ß√µes de vers√£o
                            version_info = {
                                'cpe': cpe_uri,
                                'vendor': vendor,
                                'product': product,
                                'version': version if version and version != '*' and version != '-' else None,
                                'vulnerable': vulnerable,
                                'version_start_including': cpe_match.get('versionStartIncluding'),
                                'version_start_excluding': cpe_match.get('versionStartExcluding'),
                                'version_end_including': cpe_match.get('versionEndIncluding'),
                                'version_end_excluding': cpe_match.get('versionEndExcluding')
                            }
                            
                            # Adicionar apenas se tiver informa√ß√µes de vers√£o relevantes
                            if (version_info['version'] or 
                                version_info['version_start_including'] or 
                                version_info['version_start_excluding'] or 
                                version_info['version_end_including'] or 
                                version_info['version_end_excluding']):
                                version_ranges.append(version_info)
        
        return list(vendors), list(products), version_ranges
    
    def extract_from_description(self, description):
        """Extrai vendors e products da descri√ß√£o do CVE"""
        vendors = set()
        products = set()
        
        if not description:
            return [], []
        
        description_lower = description.lower()
        
        # Buscar vendors conhecidos
        for pattern, vendor in self.vendor_patterns.items():
            if re.search(pattern, description_lower, re.IGNORECASE):
                vendors.add(vendor)
        
        # Buscar products conhecidos
        for pattern, product in self.product_patterns.items():
            if re.search(pattern, description_lower, re.IGNORECASE):
                products.add(product)
        
        return list(vendors), list(products)
    
    def extract_from_references(self, references):
        """Extrai vendors e products das refer√™ncias do CVE"""
        vendors = set()
        products = set()
        
        for ref in references:
            url = ref.get('url', '')
            if not url:
                continue
            
            url_lower = url.lower()
            
            # Extrair vendor do dom√≠nio
            domain_patterns = {
                r'microsoft\.com': 'microsoft',
                r'apple\.com': 'apple',
                r'oracle\.com': 'oracle',
                r'google\.com': 'google',
                r'debian\.org': 'debian',
                r'ubuntu\.com': 'canonical',
                r'redhat\.com': 'redhat',
                r'centos\.org': 'centos',
                r'fedoraproject\.org': 'fedoraproject',
                r'suse\.com': 'suse',
                r'opensuse\.org': 'opensuse',
                r'cisco\.com': 'cisco',
                r'ibm\.com': 'ibm',
                r'intel\.com': 'intel',
                r'amd\.com': 'amd',
                r'nvidia\.com': 'nvidia',
                r'adobe\.com': 'adobe',
                r'mozilla\.org': 'mozilla',
                r'php\.net': 'php',
                r'python\.org': 'python',
                r'nodejs\.org': 'nodejs',
                r'docker\.com': 'docker',
                r'kubernetes\.io': 'kubernetes',
                r'jenkins\.io': 'jenkins',
                r'apache\.org': 'apache',
                r'nginx\.org': 'nginx',
                r'mysql\.com': 'mysql',
                r'postgresql\.org': 'postgresql',
                r'mongodb\.com': 'mongodb',
                r'redis\.io': 'redis',
                r'elastic\.co': 'elastic',
                r'vmware\.com': 'vmware',
                r'citrix\.com': 'citrix',
                r'wordpress\.org': 'wordpress',
                r'drupal\.org': 'drupal',
                r'joomla\.org': 'joomla'
            }
            
            for pattern, vendor in domain_patterns.items():
                if re.search(pattern, url_lower):
                    vendors.add(vendor)
        
        return list(vendors), list(products)
    
    def extract_all(self, cve_data):
        """Extrai vendors, products e informa√ß√µes de vers√£o de todas as fontes dispon√≠veis"""
        all_vendors = set()
        all_products = set()
        all_version_ranges = []
        
        # 1. Extrair de configura√ß√µes CPE
        configurations = cve_data.get('configurations', [])
        cpe_vendors, cpe_products, version_ranges = self.extract_from_cpe(configurations)
        all_vendors.update(cpe_vendors)
        all_products.update(cpe_products)
        all_version_ranges.extend(version_ranges)
        
        # 2. Extrair da descri√ß√£o
        descriptions = cve_data.get('descriptions', [])
        if descriptions:
            description = descriptions[0].get('value', '')
            desc_vendors, desc_products = self.extract_from_description(description)
            all_vendors.update(desc_vendors)
            all_products.update(desc_products)
        
        # 3. Extrair das refer√™ncias
        references = cve_data.get('references', [])
        ref_vendors, ref_products = self.extract_from_references(references)
        all_vendors.update(ref_vendors)
        all_products.update(ref_products)
        
        return list(all_vendors), list(all_products), all_version_ranges


class NVDFetcher:
    """
    Sincroniza CVEs da NVD API.

    Esta classe √© respons√°vel por buscar dados da API, gerenciar cache,
    rate limiting e retries. A l√≥gica de persist√™ncia no banco de dados
    DEVE ser delegada a um servi√ßo de persist√™ncia separado.
    """

    # Adicionado type hinting para config
    def __init__(self, session: aiohttp.ClientSession, config: Dict[str, Any]):
        """
        Inicializa o NVDFetcher.

        Args:
            session: aiohttp.ClientSession para requisi√ß√µes HTTP ass√≠ncronas.
            config: Dicion√°rio com as configura√ß√µes necess√°rias (API_BASE, API_KEY, PAGE_SIZE, etc.).
        """
        self.session = session
        # TODO: Remover self.db_session ap√≥s mover a l√≥gica de persist√™ncia para um servi√ßo
        # self.db_session = db_session # Mover DB session para o servi√ßo
        self.config = config
        # Obter configura√ß√µes da API do dicion√°rio config
        self.api_base = self.config.get("NVD_API_BASE", "https://services.nvd.nist.gov/rest/json/cves/2.0")
        self.api_key = self.config.get("NVD_API_KEY")
        self.page_size = self.config.get("NVD_PAGE_SIZE", 2000)
        self.max_retries = self.config.get("NVD_MAX_RETRIES", 5)
        self.cache_dir = Path(self.config.get("NVD_CACHE_DIR", "cache"))
        self.request_timeout = self.config.get("NVD_REQUEST_TIMEOUT", 30)
        self.user_agent = self.config.get("NVD_USER_AGENT", "Sec4all.co NVD Fetcher")
        # Janela m√°xima permitida pela NVD para consultas por lastModified (em dias)
        self.max_window_days = int(self.config.get("NVD_MAX_WINDOW_DAYS", 120))

        self.headers = {"User-Agent": self.user_agent}
        if self.api_key:
             self.headers["X-API-Key"] = self.api_key

        # Inicializar o rate limiter avan√ßado
        self.rate_limiter = NVDRateLimiter.create_for_nvd(has_api_key=bool(self.api_key))
        
        # Manter compatibilidade com c√≥digo legado (ser√° removido)
        self.rate_limit_requests, self.rate_limit_window = self.config.get("NVD_RATE_LIMIT", (2, 1))
        self.request_times: List[float] = [] # Lista para controle de rate limit (legado)
        
        # Inicializar o parser aprimorado
        self.enhanced_parser = EnhancedCPEParser()
        
        # Inicializar melhorias para CWE e refer√™ncias
        self.cwe_auto_mapper = CWEAutoMapper()
        self.enhanced_reference_processor = EnhancedReferenceProcessor()


    async def validate_key(self) -> bool:
        """
        Valida se a chave da API est√° funcionando (opcional)
        tentando buscar uma p√°gina m√≠nima.
        """
        # TODO: Obter URL de valida√ß√£o da configura√ß√£o
        url = f"{self.api_base}?resultsPerPage=1" # Usar self.api_base
        logger.debug(f"Validating API key using URL: {url}")
        try:
            # Usar timeout da configura√ß√£o
            async with self.session.get(url, headers=self.headers, timeout=self.request_timeout) as resp: # Usar self.request_timeout
                # A valida√ß√£o da chave pode n√£o dar 200 mesmo com chave v√°lida,
                # mas 401 (Unauthorized) geralmente indica chave inv√°lida.
                if resp.status == 401:
                     logger.error("API key invalid or missing (401 Unauthorized).")
                     return False
                resp.raise_for_status() # Lan√ßa exce√ß√£o para outros 4xx/5xx respostas

                logger.info("API key validated successfully.")
                return True
        except aiohttp.ClientError as e:
            logger.error(f"Network or Client error during API key validation: {e}", exc_info=True)
            return False
        except Exception as e:
            logger.error(f"An unexpected error occurred during API key validation: {e}", exc_info=True)
            return False


    async def fetch_page(self, start_index: int, last_modified_start: Optional[str], last_modified_end: Optional[str] = None) -> Optional[Dict]:
        """
        Busca uma p√°gina de vulnerabilidades da API NVD.

        Args:
            start_index: √çndice inicial para a busca.
            last_modified_start: String ISO 8601 para buscar CVEs modificados
                                 DESDE esta data (usando lastModifiedStart).
                                 None para busca completa.

        Returns:
            Dicion√°rio com os dados da p√°gina da API, ou None em caso de falha.
        """
        # --- Controle de rate limit avan√ßado ---
        await self.rate_limiter.acquire()

        # --- Cacheamento b√°sico em arquivo ---
        self.cache_dir.mkdir(exist_ok=True) # Usar self.cache_dir
        # Garante que o nome do cache √© √∫nico para buscas full vs modificadas
        if last_modified_start:
            safe_start = last_modified_start.replace(":", "").replace("+", "_")
            safe_end = (last_modified_end or "now").replace(":", "").replace("+", "_") if last_modified_end else "now"
            cache_key_suffix = f"{safe_start}_{safe_end}"
        else:
            cache_key_suffix = "full"
        cache_file = self.cache_dir / f"page_{start_index}_{cache_key_suffix}.pkl" # Usar self.cache_dir

        if cache_file.exists():
            logger.debug(f"Loading page {start_index} (modified since {last_modified_start}) from cache: {cache_file}")
            try:
                data = pickle.loads(cache_file.read_bytes())
                # Opcional: adicionar valida√ß√£o b√°sica dos dados cacheados
                if isinstance(data, dict) and 'vulnerabilities' in data and 'totalResults' in data:
                    # TODO: Adicionar valida√ß√£o mais robusta do formato dos dados cacheados
                    logger.debug(f"Cache file {cache_file} loaded successfully.")
                    return data
                else:
                     logger.warning(f"Cache file {cache_file} seems incomplete or corrupted. Deleting.")
                     cache_file.unlink()
            except (pickle.UnpicklingError, EOFError) as e:
                 logger.warning(f"Error loading cache file {cache_file}: {e}. Deleting and refetching.", exc_info=True)
                 cache_file.unlink(missing_ok=True)
            except Exception as e:
                 logger.error(f"Unexpected error processing cache file {cache_file}: {e}", exc_info=True)
                 cache_file.unlink(missing_ok=True)

        # --- Monta a URL da API ---
        url = f"{self.api_base}?startIndex={start_index}&resultsPerPage={self.page_size}" # Usar self.api_base, self.page_size

        # L√≥gica CORRETA para busca incremental (CVEs modificados DESDE a √∫ltima sincroniza√ß√£o)
        # A API NVD usa lastModifiedStart e lastModifiedEnd para intervalos.
        # Se last_modified_start √© a data/hora da √∫ltima sincroniza√ß√£o, us√°-lo com lastModifiedStart
        # busca CVEs modificados NESTE timestamp ou DEPOIS.
        if last_modified_start:
            # A API NVD requer AMBOS lastModStartDate E lastModEndDate
            # Formato correto: ISO com Z (Zulu time) - exemplo: 2025-09-20T14:35:44Z
            current_time = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
            # Converte last_modified_start para o formato correto se necess√°rio
            if last_modified_start.endswith('+00:00'):
                last_modified_start = last_modified_start.replace('+00:00', 'Z')
            elif not last_modified_start.endswith('Z'):
                # Se n√£o tem timezone, assume UTC e adiciona Z
                if 'T' in last_modified_start and '+' not in last_modified_start:
                    last_modified_start += 'Z'

            # Determinar o fim da janela
            end_param = last_modified_end or current_time
            if end_param.endswith('+00:00'):
                end_param = end_param.replace('+00:00', 'Z')
            elif not end_param.endswith('Z'):
                if 'T' in end_param and '+' not in end_param:
                    end_param += 'Z'

            url += f"&lastModStartDate={last_modified_start}&lastModEndDate={end_param}"

        if last_modified_start:
            logger.info(f"Fetching page {start_index} from NVD API (range {last_modified_start} to {last_modified_end or 'now'}). URL: {url}")
        else:
            logger.info(f"Fetching page {start_index} from NVD API (full sync). URL: {url}")

        # --- Tenta buscar a p√°gina com retries ---
        for attempt in range(self.max_retries): # Usar self.max_retries
            try:
                # Adiciona cabe√ßalho de API Key condicionalmente
                headers_with_key = {**self.headers} # Copia cabe√ßalhos base (inclui User-Agent)
                if self.api_key:
                     headers_with_key["X-API-Key"] = self.api_key # Adiciona API Key se presente

                async with self.session.get(url, headers=headers_with_key, timeout=self.request_timeout) as resp: # Usar headers_with_key, self.request_timeout
                    logger.debug(f"API Response Status for page {start_index}: {resp.status}")

                    if resp.status == 200:
                        data = await resp.json()
                        # Salvar cache somente se a requisi√ß√£o foi bem-sucedida
                        try:
                            cache_file.write_bytes(pickle.dumps(data))
                            logger.debug(f"Successfully fetched and cached page {start_index}.")
                        except Exception as cache_err:
                             logger.warning(f"Failed to write cache file {cache_file}: {cache_err}", exc_info=True)
                        # TODO: Log da chamada de API (ApiCallLog) aqui?
                        return data

                    elif resp.status == 400: # Bad Request - geralmente erro na requisi√ß√£o
                         logger.error(f"API returned 400 Bad Request for page {start_index}. Check URL/params. Response: {await resp.text()}")
                         return None # Erro fatal de requisi√ß√£o

                    elif resp.status == 401: # Unauthorized - API Key inv√°lida
                         logger.error("API Key invalid or missing (401 Unauthorized). Cannot proceed.")
                         return None # Erro fatal

                    elif resp.status == 404: # Not Found
                         logger.warning(f"API returned 404 Not Found for page {start_index}. URL may be incorrect or no data.")
                         # Pode significar o fim dos resultados em alguns casos, mas a API geralmente retorna 200 com lista vazia.
                         # Tratar como um erro que interrompe o processo por seguran√ßa, a menos que se saiba o contr√°rio.
                         return None

                    elif resp.status == 429: # Rate Limit Exceeded
                        # Usar o sistema avan√ßado de rate limiting para tratar 429
                        should_retry = await self.rate_limiter.handle_http_error(
                            resp.status, dict(resp.headers)
                        )
                        if should_retry and attempt < self.max_retries - 1:
                            continue
                        else:
                            return None

                    elif resp.status >= 500: # Server Error
                        # Usar o sistema avan√ßado de rate limiting para tratar erros de servidor
                        should_retry = await self.rate_limiter.handle_http_error(
                            resp.status, dict(resp.headers)
                        )
                        if should_retry and attempt < self.max_retries - 1:
                            continue
                        else:
                            return None

                    else: # Outro erro inesperado
                        logger.error(f"Unexpected API Error {resp.status} for page {start_index} (Attempt {attempt + 1}/{self.max_retries}). Response: {await resp.text()}") # Usar self.max_retries
                        if attempt == self.max_retries - 1: # Usar self.max_retries
                            return None # N√£o tenta novamente ap√≥s o √∫ltimo retry
                        else:
                             wait_time = 2**(attempt + 1)
                             await asyncio.sleep(wait_time) # Espera antes de tentar novamente
                             continue

            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                # Erros de conex√£o, timeout ou requisi√ß√£o com aiohttp
                logger.error(f"Network, Timeout, or Client Error fetching page {start_index} (Attempt {attempt + 1}/{self.max_retries}): {e}", exc_info=True) # Usar self.max_retries
                if attempt == self.max_retries - 1: # Usar self.max_retries
                    return None # N√£o tenta novamente ap√≥s o √∫ltimo retry
                else:
                    # Backoff exponencial + um pouco mais para garantir tempo suficiente
                    wait_time = 2**(attempt + 1) + 1 # Adiciona 1 segundo extra
                    logger.warning(f"Retrying fetch for page {start_index} in {wait_time:.2f} seconds.")
                    await asyncio.sleep(wait_time) # Espera antes de tentar novamente
                    continue
            except Exception as e:
                # Outros erros inesperados durante o fetch
                 logger.error(f"An unexpected error occurred during fetch_page for page {start_index} (Attempt {attempt + 1}/{self.max_retries}): {e}", exc_info=True) # Usar self.max_retries
                 return None # Erro inesperado, parar

        logger.error(f"Failed to fetch page {start_index} after {self.max_retries} attempts.") # Usar self.max_retries
        # TODO: Log da falha no ApiCallLog?
        return None # Falhou ap√≥s todas as tentativas


    # TODO: Esta l√≥gica de processamento e mapeamento DEVE ser movida para o VulnerabilityService.
    # O fetcher deve retornar os dados brutos ou um dicion√°rio leve.
    async def process_cve_data(self, cve_data: Dict[str, Any]) -> Optional[Dict[str, Any]]: # Alterar retorno para Dict ou DTO
        """
        Processa os dados brutos de um √∫nico CVE da API NVD e retorna um dicion√°rio/DTO.
        Esta l√≥gica deveria estar em um SERVI√áO ou REPOSIT√ìRIO de Vulnerabilidades.
        """
        # Exemplo de como extrair e mapear dados (ajuste conforme sua estrutura de modelo)
        cve_id = cve_data.get('id')
        if not cve_id:
            logger.warning("Skipping CVE item with no 'id'.")
            return None

        # Extrair descri√ß√£o em ingl√™s
        description = "No description available."
        for desc in cve_data.get('descriptions', []):
            if desc.get('lang') == 'en':
                 description = desc.get('value', description)
                 break

        # Extrair datas (lidar com formato ISO 8601, pode ter timezone)
        published_str = cve_data.get('published')
        last_modified_str = cve_data.get('lastModified')

        published_date = None
        if published_str:
            try:
                 # Ex: '2023-10-01T13:00:00.000-05:00' ou '2023-10-01T18:00:00Z'
                 published_date = datetime.fromisoformat(published_str.replace('Z', '+00:00'))
            except ValueError:
                 logger.warning(f"Could not parse published date '{published_str}' for CVE {cve_id}.")

        last_modified = None
        if last_modified_str:
            try:
                 last_modified = datetime.fromisoformat(last_modified_str.replace('Z', '+00:00'))
            except ValueError:
                 logger.warning(f"Could not parse last modified date '{last_modified_str}' for CVE {cve_id}.")

        # Extrair campos adicionais da API NVD 2.0
        source_identifier = cve_data.get('sourceIdentifier')
        vuln_status = cve_data.get('vulnStatus')
        evaluator_comment = cve_data.get('evaluatorComment')
        evaluator_solution = cve_data.get('evaluatorSolution')
        evaluator_impact = cve_data.get('evaluatorImpact')
        
        # Extrair campos CISA KEV (Known Exploited Vulnerabilities)
        cisa_exploit_add = None
        cisa_action_due = None
        cisa_required_action = cve_data.get('cisaRequiredAction')
        cisa_vulnerability_name = cve_data.get('cisaVulnerabilityName')
        
        # Processar datas CISA se dispon√≠veis
        cisa_exploit_add_str = cve_data.get('cisaExploitAdd')
        if cisa_exploit_add_str:
            try:
                cisa_exploit_add = datetime.fromisoformat(cisa_exploit_add_str.replace('Z', '+00:00'))
            except ValueError:
                logger.warning(f"Could not parse CISA exploit add date '{cisa_exploit_add_str}' for CVE {cve_id}.")
        
        cisa_action_due_str = cve_data.get('cisaActionDue')
        if cisa_action_due_str:
            try:
                cisa_action_due = datetime.fromisoformat(cisa_action_due_str.replace('Z', '+00:00'))
            except ValueError:
                logger.warning(f"Could not parse CISA action due date '{cisa_action_due_str}' for CVE {cve_id}.")

        # Extrair m√©tricas CVSS de todas as vers√µes dispon√≠veis
        base_severity = 'N/A'
        cvss_score = None
        cvss_metrics = []

        metrics = cve_data.get('metrics', {})
        # Mapear chaves CVSS para vers√µes
        cvss_version_map = {
            'cvssMetricV31': '3.1',
            'cvssMetricV30': '3.0', 
            'cvssMetricV2': '2.0'
        }
        
        # Ordem de prioridade para score principal
        priority_order = ['cvssMetricV31', 'cvssMetricV30', 'cvssMetricV2']
        
        # Extrair todas as m√©tricas CVSS dispon√≠veis
        for cvss_key, version in cvss_version_map.items():
            metric_list = metrics.get(cvss_key, [])
            if not isinstance(metric_list, list):
                continue
                
            for i, metric_item in enumerate(metric_list):
                cvss_data = metric_item.get('cvssData', {})
                if not cvss_data:
                    continue
                    
                try:
                    # Extrair score CVSS
                    base_score = float(cvss_data.get('baseScore', 0.0))
                    
                    # Extrair severidade - com fallback para CVEs antigos
                    severity_value = cvss_data.get('baseSeverity', '').upper()
                    
                    # Se n√£o tem baseSeverity (CVEs antigos), mapear do score
                    if not severity_value or severity_value in ['UNKNOWN', '']:
                        severity_value = map_cvss_score_to_severity(base_score, version)
                    
                    # Validar severidade final
                    if severity_value not in ['NONE', 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL']:
                        severity_value = 'N/A'
                    
                    metric_info = {
                        'cvss_version': version,
                        'base_score': base_score,
                        'base_severity': severity_value,
                        'base_vector': cvss_data.get('vectorString', ''),
                        'is_primary': i == 0,  # Primeira m√©trica de cada vers√£o √© prim√°ria
                        'exploitability_score': cvss_data.get('exploitabilityScore'),
                        'impact_score': cvss_data.get('impactScore')
                    }
                    
                    # Extrair componentes espec√≠ficos por vers√£o
                    if version in ['3.0', '3.1']:
                        # CVSS v3.x componentes
                        metric_info.update({
                            'attack_vector': cvss_data.get('attackVector'),
                            'attack_complexity': cvss_data.get('attackComplexity'),
                            'privileges_required': cvss_data.get('privilegesRequired'),
                            'user_interaction': cvss_data.get('userInteraction'),
                            'scope': cvss_data.get('scope'),
                            'confidentiality_impact': cvss_data.get('confidentialityImpact'),
                            'integrity_impact': cvss_data.get('integrityImpact'),
                            'availability_impact': cvss_data.get('availabilityImpact')
                        })
                    elif version == '2.0':
                        # CVSS v2.x componentes
                        metric_info.update({
                            'access_vector': cvss_data.get('accessVector'),
                            'access_complexity': cvss_data.get('accessComplexity'),
                            'authentication': cvss_data.get('authentication'),
                            'confidentiality_impact': cvss_data.get('confidentialityImpact'),
                            'integrity_impact': cvss_data.get('integrityImpact'),
                            'availability_impact': cvss_data.get('availabilityImpact')
                        })
                    
                    # Converter scores para float quando dispon√≠veis
                    for score_field in ['exploitability_score', 'impact_score']:
                        if metric_info[score_field] is not None:
                            try:
                                metric_info[score_field] = float(metric_info[score_field])
                            except (ValueError, TypeError):
                                metric_info[score_field] = None
                    
                    cvss_metrics.append(metric_info)
                    
                except (ValueError, TypeError) as e:
                    logger.warning(f"Error processing CVSS {version} data for CVE {cve_id}: {e}")
                    continue
        
        # Definir score e severidade principais usando a fun√ß√£o utilit√°ria
        base_severity, cvss_score = get_primary_severity_from_metrics(cvss_metrics)
        
        # Fallback para o m√©todo anterior se a fun√ß√£o utilit√°ria n√£o encontrar nada
        if base_severity == 'N/A' and cvss_score is None:
            for priority_key in priority_order:
                if priority_key in cvss_version_map:
                    version = cvss_version_map[priority_key]
                    primary_metrics = [m for m in cvss_metrics if m['cvss_version'] == version and m['is_primary']]
                    if primary_metrics:
                        primary_metric = primary_metrics[0]
                        cvss_score = primary_metric['base_score']
                        base_severity = primary_metric['base_severity']
                        break

        # Extrair dados de vendors, products e informa√ß√µes de vers√£o usando o parser aprimorado
        vendors_data, products_data, version_ranges_data = self.enhanced_parser.extract_all(cve_data)
        
        # Extrair CWEs usando o m√©todo original e adicionar mapeamento autom√°tico
        weaknesses_data = self._extract_weaknesses(cve_data.get('weaknesses', []))
        
        # Aplicar mapeamento autom√°tico de CWEs baseado na descri√ß√£o
        auto_mapped_cwes = self.cwe_auto_mapper.map_cwe_from_description(description)
        
        # Combinar CWEs expl√≠citos com os mapeados automaticamente
        all_cwes = set(weaknesses_data)  # CWEs expl√≠citos
        all_cwes.update(auto_mapped_cwes)  # Adicionar CWEs mapeados automaticamente
        weaknesses_data = list(all_cwes)
        
        # Processar refer√™ncias com melhorias
        references_data = []
        patch_available = False
        patch_references = []
        
        references = cve_data.get('references', [])
        for ref in references:
            url = ref.get('url')
            if url:
                ref_tags = ref.get('tags', [])
                
                # Validar refer√™ncia usando o processador aprimorado
                if self.enhanced_reference_processor.validate_reference(url):
                    references_data.append({
                        'url': url,
                        'source': ref.get('source', ''),
                        'tags': ref_tags
                    })
                    
                    # Detectar patches usando o processador aprimorado
                    ref_dict = {'url': url, 'tags': ref_tags}
                    if self.enhanced_reference_processor.enhanced_patch_detection(ref_dict):
                        patch_available = True
                        patch_references.append({
                            'url': url,
                            'source': ref.get('source', ''),
                            'tags': ref_tags
                        })

        # Log informativo sobre patches e CWEs detectados
        if patch_available:
            logger.debug(f"CVE {cve_id}: Patch detected from {len(patch_references)} reference(s)")
        if auto_mapped_cwes:
            logger.debug(f"CVE {cve_id}: Auto-mapped CWEs: {auto_mapped_cwes}")
        
        # Em uma refatora√ß√£o completa, isto retornaria um dicion√°rio ou DTO
        # que representa os dados prontos para serem passados para o servi√ßo de persist√™ncia.
        extracted_data = {
             "cve_id": cve_id,
             "description": description,
             "published_date": published_date,
             "last_update": last_modified,
             "base_severity": base_severity,
             "cvss_score": cvss_score if cvss_score is not None else 0.0,
             "patch_available": patch_available,  # Agora baseado em dados reais da API
             "cvss_metrics": cvss_metrics,  # Lista completa de m√©tricas CVSS
             "vendors": vendors_data,
             "products": products_data,
             "weaknesses": weaknesses_data,
             "references": references_data,  # Adicionar refer√™ncias
             "version_ranges": version_ranges_data,  # Informa√ß√µes de vers√£o extra√≠das das configura√ß√µes CPE
             "cpe_configurations": cve_data.get('configurations', []),  # Configura√ß√µes CPE completas para refer√™ncia
             
             # Campos adicionais da API NVD 2.0
             "source_identifier": source_identifier,
             "vuln_status": vuln_status,
             "evaluator_comment": evaluator_comment,
             "evaluator_solution": evaluator_solution,
             "evaluator_impact": evaluator_impact,
             
             # Campos CISA KEV (Known Exploited Vulnerabilities)
             "cisa_exploit_add": cisa_exploit_add,
             "cisa_action_due": cisa_action_due,
             "cisa_required_action": cisa_required_action,
             "cisa_vulnerability_name": cisa_vulnerability_name
        }
        # NOVO: Retornar o dicion√°rio de dados extra√≠dos/mapeados
        return extracted_data

    # M√©todos de extra√ß√£o removidos - agora usando EnhancedCPEParser

    def _extract_weaknesses(self, weaknesses):
        """Extrai CWE IDs das weaknesses com melhor cobertura."""
        cwe_ids = set()
        
        # Prioridade de idiomas (ingl√™s primeiro, depois outros)
        lang_priority = ['en', 'es', 'fr', 'de', 'pt', 'it']
        
        for weakness in weaknesses:
            descriptions = weakness.get('description', [])
            
            # Tentar extrair CWE em ordem de prioridade de idioma
            extracted = False
            for lang in lang_priority:
                if extracted:
                    break
                    
                for desc in descriptions:
                    if desc.get('lang') == lang:
                        value = desc.get('value', '').strip()
                        
                        # Formato padr√£o CWE-XXX
                        if value.startswith('CWE-') and len(value) > 4:
                            # Validar que ap√≥s CWE- h√° n√∫meros
                            cwe_number = value[4:]
                            if cwe_number.isdigit():
                                cwe_ids.add(value)
                                extracted = True
                                break
            
            # Se n√£o encontrou CWE v√°lido, tentar em qualquer idioma
            if not extracted:
                for desc in descriptions:
                    value = desc.get('value', '').strip()
                    if value.startswith('CWE-') and len(value) > 4:
                        cwe_number = value[4:]
                        if cwe_number.isdigit():
                            cwe_ids.add(value)
                            break
        
        return list(cwe_ids)
    
    def _validate_cve_data(self, cve_data: Dict[str, Any]) -> bool:
        """
        Valida os dados de uma CVE antes da grava√ß√£o no banco.
        
        Args:
            cve_data: Dicion√°rio com dados da CVE
            
        Returns:
            bool: True se os dados s√£o v√°lidos, False caso contr√°rio
        """
        try:
            # Valida√ß√µes obrigat√≥rias
            if not cve_data.get('cve_id'):
                terminal_feedback.error("‚ùå CVE ID √© obrigat√≥rio")
                return False
            
            # Validar formato do CVE ID
            cve_id = cve_data['cve_id']
            if not cve_id.startswith('CVE-') or len(cve_id) < 8:
                terminal_feedback.error(f"‚ùå Formato inv√°lido de CVE ID: {cve_id}")
                return False
            
            # Validar descri√ß√£o
            description = cve_data.get('description', '')
            if not description or description == 'No description available.':
                terminal_feedback.warning(f"‚ö†Ô∏è CVE {cve_id} sem descri√ß√£o v√°lida")
            
            # Validar CVSS score
            cvss_score = cve_data.get('cvss_score')
            if cvss_score is not None and (cvss_score < 0 or cvss_score > 10):
                terminal_feedback.warning(f"‚ö†Ô∏è CVE {cve_id} com CVSS score inv√°lido: {cvss_score}")
            
            # Validar datas
            published_date = cve_data.get('published_date')
            if published_date and not isinstance(published_date, datetime):
                terminal_feedback.warning(f"‚ö†Ô∏è CVE {cve_id} com data de publica√ß√£o inv√°lida")
            
            # Validar severidade
            base_severity = cve_data.get('base_severity', '')
            valid_severities = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'N/A']
            if base_severity not in valid_severities:
                terminal_feedback.warning(f"‚ö†Ô∏è CVE {cve_id} com severidade inv√°lida: {base_severity}")
            
            # Validar estruturas de dados
            for field in ['cvss_metrics', 'vendors', 'products', 'weaknesses', 'references']:
                if field in cve_data and not isinstance(cve_data[field], list):
                    terminal_feedback.warning(f"‚ö†Ô∏è CVE {cve_id}: campo {field} deve ser uma lista")
            
            return True
            
        except Exception as e:
            terminal_feedback.error(f"‚ùå Erro na valida√ß√£o da CVE {cve_data.get('cve_id', 'unknown')}: {str(e)}")
            return False

    # TODO: Modificar o m√©todo update para receber uma inst√¢ncia do VulnerabilityService
    # e delegar a ele a l√≥gica de persist√™ncia.
    async def update(self, vulnerability_service: 'VulnerabilityService', full: bool = False) -> int:
        """
        Atualiza o banco de dados com as √∫ltimas vulnerabilidades da NVD
        usando um servi√ßo de persist√™ncia com feedback aprimorado.

        Args:
            vulnerability_service: Inst√¢ncia do servi√ßo respons√°vel pela persist√™ncia.
            full: Se True, ignora a √∫ltima data de sincroniza√ß√£o e busca todos os CVEs.

        Returns:
            O n√∫mero total de vulnerabilidades processadas (salvas no DB).
        """
        # Usar sistema de feedback aprimorado
        sync_type = "completa" if full else "incremental"
        terminal_feedback.info(f"üîÑ Iniciando sincroniza√ß√£o NVD ({sync_type})", 
                             {"type": sync_type, "timestamp": datetime.now().isoformat()})

        # Exibir estat√≠sticas iniciais
        nvd_stats.display_comprehensive_stats()
        
        # Obter logger aprimorado
        enhanced_logger = get_app_logger()
        
        # MONITORAMENTO DE MEM√ìRIA: Inicializar monitoramento
        memory_monitor.log_memory_status("in√≠cio da sincroniza√ß√£o")
        system_memory = memory_monitor.get_system_memory_info()
        logger.info(f"Mem√≥ria do sistema: {system_memory.get('available_gb', 0):.1f}GB dispon√≠vel de {system_memory.get('total_gb', 0):.1f}GB total")
        
        # TODO: Mover a l√≥gica de acesso a SyncMetadata para o VulnerabilityService.
        last_synced_time = None
        last_synced_time_str = None
        
        with enhanced_logger.operation_context("Verifica√ß√£o de sincroniza√ß√£o anterior") as op_id:
            if not full:
                 # Buscar a √∫ltima data de sincroniza√ß√£o usando o servi√ßo
                 last_synced_time = vulnerability_service.get_last_sync_time()
                 if last_synced_time:
                     # Converter datetime para string ISO 8601 no formato que a API espera (lastModifiedStart)
                     last_synced_time_str = last_synced_time.isoformat(timespec='milliseconds').replace('+00:00', 'Z')
                     terminal_feedback.success(f"√öltima sincroniza√ß√£o encontrada: {last_synced_time_str}", 
                                             {"last_sync": last_synced_time_str})
                 else:
                     terminal_feedback.warning("Nenhuma sincroniza√ß√£o anterior encontrada", 
                                             {"action": "Executando sincroniza√ß√£o inicial"})
            else:
                 terminal_feedback.info("Sincroniza√ß√£o completa solicitada", 
                                      {"ignore_last_sync": True})
            
            enhanced_logger.update_operation(op_id, progress=1.0, details="Verifica√ß√£o conclu√≠da")


        total_processed = 0 # Total de itens processados E SALVOS no DB
        start_index = 0
        total_results_expected = None # Para rastrear o total de resultados para a query


        # Usar um try/finally para garantir o rollback ou commit da sess√£o NO SERVI√áO
        # A sess√£o √© gerenciada pelo servi√ßo. O fetcher n√£o faz commit/rollback direto.
        try:
            # Preparar janelas de tempo quando incremental excede o limite da NVD
            windows: List[Dict[str, str]] = []
            if not full and last_synced_time:
                # Converter para timezone UTC consciente
                last_dt = last_synced_time if last_synced_time.tzinfo else last_synced_time.replace(tzinfo=timezone.utc)
                last_dt = last_dt.astimezone(timezone.utc)
                now_dt = datetime.now(timezone.utc)
                # Construir janelas de at√© max_window_days
                if (now_dt - last_dt).days > self.max_window_days:
                    logger.warning(
                        f"Janela incremental de {(now_dt - last_dt).days} dias excede o limite de {self.max_window_days} dias. Aplicando chunking por janelas.")
                    cursor = last_dt
                    while cursor <= now_dt:
                        window_end = min(cursor + timedelta(days=self.max_window_days), now_dt)
                        windows.append({
                            'start': cursor.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                            'end': window_end.isoformat(timespec='milliseconds').replace('+00:00', 'Z')
                        })
                        # Avan√ßa 1 segundo para evitar reprocessar o mesmo registro no limite inclusivo
                        cursor = window_end + timedelta(seconds=1)
                else:
                    # Uma √∫nica janela dentro do limite
                    windows.append({
                        'start': last_dt.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                        'end': now_dt.isoformat(timespec='milliseconds').replace('+00:00', 'Z')
                    })

            # Indicador de sucesso global
            global_success = True

            # Se n√£o √© incremental (full) ou n√£o h√° last_sync, executar loop √∫nico
            if full or not windows:
                windows = [{'start': None, 'end': None}]  # Sem filtros de data

            # Processar cada janela separadamente com pagina√ß√£o pr√≥pria
            for idx, win in enumerate(windows):
                start_index = 0
                total_results_expected = None
                win_start = win['start']
                win_end = win['end']
                if win_start and win_end:
                    terminal_feedback.info(
                        f"üóìÔ∏è Processando janela {idx+1}/{len(windows)}: {win_start} ‚Üí {win_end}",
                        {"window_index": idx+1, "windows_total": len(windows)}
                    )

                while True:
                    # Passar par√¢metros de janela ao fetch_page
                    data = await self.fetch_page(start_index, win_start, win_end)

                    if data is None:
                        logger.error("Failed to fetch data from NVD API. Stopping update process.")
                        # N√£o atualiza a data de sincroniza√ß√£o em caso de falha fatal no fetch
                        global_success = False
                        break  # Sair do loop da janela atual

                    vulnerabilities_data_raw = data.get('vulnerabilities', [])
                    total_results_on_api = data.get('totalResults', 0)

                    if total_results_expected is None:
                        total_results_expected = total_results_on_api
                        logger.info(f"Total results for this query range expected: {total_results_expected}")
                        # TODO: Opcional: Usar tqdm com o total esperado
                        # pbar = tqdm.tqdm(total=total_results_expected, desc='Processing CVEs')

                    if not vulnerabilities_data_raw:
                        # Se a p√°gina atual retornou vazio e n√£o h√° mais resultados esperados (ou chegamos ao fim te√≥rico)
                        # A API geralmente retorna totalResults > current_index mesmo na √∫ltima p√°gina com dados,
                        # mas √© mais seguro verificar se a lista est√° vazia E chegamos ou passamos o total esperado.
                        if start_index >= total_results_expected:  # Usar total_results_expected para controle
                            logger.info("Reached end of results for current window.")
                            break  # Sair do loop se n√£o houver mais dados nesta p√°gina E for o fim esperado
                        else:
                            # Se a p√°gina atual retornou vazio, mas ainda h√° resultados esperados (API inconsist√™ncia?)
                            logger.warning(
                                f"Page {start_index} returned no vulnerabilities, but total results suggest more data ({total_results_expected}). API issue? Stopping for safety.")
                            break  # Parar por seguran√ßa

                    # OTIMIZA√á√ÉO DE MEM√ìRIA: Processar dados em lotes menores para reduzir uso de mem√≥ria
                    import gc  # Para garbage collection expl√≠cito

                    MEMORY_BATCH_SIZE = 50  # Processar em lotes menores para evitar estouro de mem√≥ria

                    # Processar vulnerabilidades em mini-lotes para otimizar mem√≥ria
                    for i in range(0, len(vulnerabilities_data_raw), MEMORY_BATCH_SIZE):
                        batch = vulnerabilities_data_raw[i:i + MEMORY_BATCH_SIZE]

                        # Processar lote atual
                        processed_data_list = []
                        for item in batch:
                            # process_cve_data AGORA retorna um dicion√°rio/DTO, n√£o um objeto ORM
                            extracted_data = await self.process_cve_data(item.get('cve'))
                            if extracted_data:  # Se a extra√ß√£o/mapeamento foi bem-sucedido
                                processed_data_list.append(extracted_data)

                        # Salvar mini-lote imediatamente para liberar mem√≥ria
                        if processed_data_list:
                            try:
                                # Validar dados antes de salvar
                                valid_data = []
                                for data in processed_data_list:
                                    if self._validate_cve_data(data):
                                        valid_data.append(data)
                                    else:
                                        terminal_feedback.warning(
                                            f"‚ö†Ô∏è Dados inv√°lidos para CVE {data.get('cve_id', 'unknown')}")

                                if valid_data:
                                    # O servi√ßo lida com a cria√ß√£o/atualiza√ß√£o dos objetos ORM e o commit em lote.
                                    processed_count_batch = vulnerability_service.save_vulnerabilities_batch(valid_data)
                                    total_processed += processed_count_batch  # Acumula o total processado PELO SERVI√áO

                                    # Feedback de progresso
                                    terminal_feedback.info(
                                        f"üì¶ Mini-lote {i//MEMORY_BATCH_SIZE + 1} processado",
                                        {
                                            "cves_processadas": processed_count_batch,
                                            "total_acumulado": total_processed,
                                            "memoria_mb": memory_monitor.get_memory_usage_mb()
                                        }
                                    )

                                    logger.debug(
                                        f"Processed mini-batch {i//MEMORY_BATCH_SIZE + 1} with {processed_count_batch} CVEs")
                                else:
                                    terminal_feedback.warning(
                                        f"‚ö†Ô∏è Nenhum dado v√°lido no mini-lote {i//MEMORY_BATCH_SIZE + 1}")

                            except Exception as service_error:
                                # Captura erros que ocorreram no servi√ßo de persist√™ncia
                                logger.error(f"Error saving vulnerability mini-batch: {service_error}", exc_info=True)
                                terminal_feedback.error(f"‚ùå Erro ao salvar mini-lote: {str(service_error)}")
                                logger.error("Stopping update due to service persistence error.")
                                raise  # Re-raise para parar o processamento

                        # Limpar refer√™ncias para liberar mem√≥ria
                        del processed_data_list
                        del batch

                        # Garbage collection peri√≥dico para mini-lotes
                        if i % (MEMORY_BATCH_SIZE * 2) == 0:
                            gc.collect()

                    # Limpar dados da p√°gina atual para liberar mem√≥ria
                    del vulnerabilities_data_raw

                    logger.info(f"Completed page starting at index {start_index}. Total processed: {total_processed}")

                    # Verificar se h√° mais p√°ginas a buscar com base no totalResults esperado
                    # Isso pode ser um pouco impreciso se o totalResults mudar durante a execu√ß√£o,
                    # mas √© uma boa heur√≠stica. A condi√ß√£o principal de sa√≠da √© quando fetch_page
                    # retorna uma lista vazia E index >= total_results_expected.
                    if start_index + self.page_size >= total_results_expected:  # Usar self.page_size
                        logger.info("Reached end of results for current window based on totalResults estimate.")
                        # Verifica se realmente n√£o h√° mais dados na pr√≥xima chamada
                        # (j√° coberto pela l√≥gica de 'not vulnerabilities_data' no in√≠cio do loop)
                        pass  # Continua o loop para a pr√≥xima fetch_page, que deve retornar vazio

                    # Avan√ßar para o pr√≥ximo √≠ndice
                    start_index += self.page_size  # Usar self.page_size

                    # OTIMIZA√á√ÉO DE MEM√ìRIA: Monitoramento e garbage collection peri√≥dico entre p√°ginas
                    page_number = start_index // self.page_size
                    if page_number % 5 == 0:  # A cada 5 p√°ginas
                        # Verificar status da mem√≥ria e executar GC se necess√°rio
                        gc_stats = memory_monitor.auto_manage_memory(f"ap√≥s p√°gina {page_number}")
                        if gc_stats:
                            logger.info(
                                f"GC autom√°tico executado: {gc_stats['objects_collected']} objetos, {gc_stats['memory_freed_mb']:.1f}MB liberados")
                        else:
                            gc.collect()  # GC regular

                        memory_monitor.log_memory_status(f"p√°gina {page_number}")
                        logger.debug(f"Executed garbage collection after {page_number} pages")

                # Adicionar um pequeno delay entre as p√°ginas, al√©m do rate limit interno do fetch_page
                # Isso pode ser √∫til se a API tiver limites por minuto/hora al√©m do por segundo.
                # await asyncio.sleep(1) # Exemplo: espera 1 segundo entre as p√°ginas


            # TODO: pbar.close() # Fechar barra de progresso

            # --- Atualizar a data da √∫ltima sincroniza√ß√£o ---
            # Atualizar a data somente se a opera√ß√£o foi bem-sucedida (sem falhas fatais no fetch ou save)
            # Uma opera√ß√£o bem-sucedida significa que o loop principal n√£o foi interrompido por um erro.
            # Podemos usar uma flag ou verificar se chegamos ao fim esperado (index >= total_results_expected ou total_results_expected == 0).
            # Sucesso se todas as janelas completaram sem falha
            sync_completed_successfully = global_success

            if sync_completed_successfully:
                 # Atualizar a data da √∫ltima sincroniza√ß√£o usando o servi√ßo
                 vulnerability_service.update_last_sync_time(datetime.utcnow()) # Passar datetime.utcnow()

            else:
                logger.warning("Sync operation did not complete successfully. Last sync time not updated via service.")

        except Exception as e:
             # Capturar quaisquer erros inesperados durante a orquestra√ß√£o do update
             logger.error("An unexpected error occurred during the NVD update process.", exc_info=True)
             # TODO: A sess√£o do servi√ßo j√° deve ter feito rollback em caso de erro
             # self.db_session.rollback() # Remover - sess√£o gerenciada pelo servi√ßo


        # MONITORAMENTO DE MEM√ìRIA: Log final de estat√≠sticas
        memory_stats = memory_monitor.get_memory_stats()
        memory_monitor.log_memory_status("final da sincroniza√ß√£o")
        logger.info(f"Estat√≠sticas de mem√≥ria - Pico: {memory_stats['peak_mb']:.1f}MB, "
                   f"Aumento: +{memory_stats['increase_mb']:.1f}MB, "
                   f"GCs executados: {memory_stats['gc_count']}")
        
        # Exibir estat√≠sticas finais
        terminal_feedback.success(f"‚úÖ Sincroniza√ß√£o NVD conclu√≠da!", 
                                {"processed": total_processed, "type": sync_type})
        
        # Mostrar estat√≠sticas atualizadas
        nvd_stats.display_comprehensive_stats()
        
        # Exibir progresso da sincroniza√ß√£o
        progress_info = nvd_stats.get_sync_progress_info()
        if progress_info and progress_info.get('is_syncing'):
            terminal_feedback.info(f"üìä Progresso da sincroniza√ß√£o:", progress_info)
        
        logger.info(f"NVD update job finished. Total vulnerabilities processed (and saved by service): {total_processed}.")
        return total_processed


# --- L√≥gica para execu√ß√£o como script standalone ---

# Mover a l√≥gica de configura√ß√£o b√°sica para uma fun√ß√£o de setup
def setup_standalone_script(env_name: str = None) -> Flask:
     """Cria e configura a aplica√ß√£o Flask para execu√ß√£o standalone."""
     # Importar a f√°brica de aplica√ß√£o principal
     from app import create_app # Importa√ß√£o CORRETA da f√°brica

     # TODO: Passar args.config para create_app se necess√°rio para carregar ambiente espec√≠fico
     #create_app j√° deve carregar a config com base no env_name
     app = create_app(env_name=env_name) # Usar a f√°brica principal com ambiente opcional

     # O contexto da aplica√ß√£o j√° √© gerenciado por create_app e o with statement
     # As extens√µes j√° s√£o inicializadas por create_app

     return app

# Adicionar parser de argumentos de linha de comando
parser = argparse.ArgumentParser(description="NVD Fetcher Job: Sync CVEs from NVD API.")
parser.add_argument(
    '--full',
    action='store_true',
    help='Perform a full synchronization instead of incremental.'
)
parser.add_argument(
    '--config',
    type=str,
    default=None,
    help='Flask environment configuration (e.g., development, production).'
)


if __name__ == '__main__':
    args = parser.parse_args()

    # Configurar e obter a aplica√ß√£o Flask
    # Passar o nome do ambiente, se fornecido via argumento --config
    app = setup_standalone_script(env_name=args.config)

    # Rodar dentro do contexto da aplica√ß√£o para ter acesso a app.config e db.session
    with app.app_context():
        # A inst√¢ncia db.session j√° est√° dispon√≠vel aqui
        # TODO: Garantir que o logging √© configurado corretamente no contexto do job standalone
        # (Pode ser feito no setup_standalone_script ou globalmente se n√£o conflitar)

        # Obter configura√ß√µes necess√°rias de app.config para passar para o fetcher
        # Acessar app.config AP√ìS criar o app e dentro do app_context
        nvd_config = {
            "NVD_API_BASE": getattr(app.config, 'NVD_API_BASE', "https://services.nvd.nist.gov/rest/json/cves/2.0"),
            "NVD_API_KEY": getattr(app.config, 'NVD_API_KEY', None),
            "NVD_PAGE_SIZE": getattr(app.config, 'NVD_PAGE_SIZE', 2000),
            "NVD_MAX_RETRIES": getattr(app.config, 'NVD_MAX_RETRIES', 5),
            "NVD_RATE_LIMIT": getattr(app.config, 'NVD_RATE_LIMIT', (2, 1)),
            "NVD_CACHE_DIR": getattr(app.config, 'NVD_CACHE_DIR', "cache"),
            "NVD_REQUEST_TIMEOUT": getattr(app.config, 'NVD_REQUEST_TIMEOUT', 30),
            "NVD_USER_AGENT": getattr(app.config, 'NVD_USER_AGENT', "Sec4all.co NVD Fetcher"), # Exemplo de outra config
            "NVD_MAX_WINDOW_DAYS": getattr(app.config, 'NVD_MAX_WINDOW_DAYS', 120),
        }

        # Validar configura√ß√µes essenciais (ex: API_BASE)
        if not nvd_config.get("NVD_API_BASE"):
             logger.error("NVD_API_BASE configuration is missing in app.config. Cannot run fetcher.")
             sys.exit(1) # Sair com c√≥digo de erro
        
        # Usar aiohttp.ClientSession dentro de um bloco async with para garantir que seja fechada
        async def run_fetcher():
            async with aiohttp.ClientSession() as http_session:
                # Instanciar o fetcher, passando a sess√£o HTTP e as configura√ß√µes
                fetcher = NVDFetcher(http_session, nvd_config)

                # Criar inst√¢ncia do VulnerabilityService (m√≥dulo dentro de app)
                from app.services.vulnerability_service import VulnerabilityService
                vulnerability_service = VulnerabilityService(db.session)

                # Rodar o update ass√≠ncrono, passando o servi√ßo
                processed_count = await fetcher.update(vulnerability_service=vulnerability_service, full=args.full)

                logger.info(f"NVD Fetcher job finished. Processed {processed_count} CVEs.")

        # Rodar a fun√ß√£o ass√≠ncrona principal
        try:
            asyncio.run(run_fetcher())
            sys.exit(0)
        except Exception as e:
            logger.error("An error occurred during the asyncio run.", exc_info=True)
            sys.exit(1) # Sair com c√≥digo de erro em caso de falha

    # Execu√ß√£o finalizada
    logger.info("Standalone script execution finished.")